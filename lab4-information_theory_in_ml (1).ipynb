{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 4 - Information Theory in Machine Learning\n",
    "\n",
    "Welcome to this week's lab on Information Theory! This week, we will dive into the fascinating world of Information Theory as applied to Machine Learning. Specifically, we will focus on two key concepts: Entropy and Information Gain. These principles are fundamental in understanding how decision trees make split decisions to organize data effectively.\n",
    "\n",
    "### Entropy\n",
    "- Entropy, in the context of information theory, measures the level of uncertainty or disorder within a set of data.\n",
    "- In machine learning, particularly in decision trees, entropy helps to determine how a dataset should be split. A high entropy means more disorder, indicating that our dataset is varied. Conversely, low entropy suggests more uniformity in the data.\n",
    "\n",
    "### Information Gain\n",
    "- Information Gain measures the reduction in entropy after the dataset is split on an attribute.\n",
    "- It is crucial in building decision trees as it helps to decide the order of attributes the tree will use for splitting the data. The attribute with the highest Information Gain is chosen as the splitting attribute at each node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Entropy and Information Gain in Decision Trees\n",
    "Decision Trees use these concepts to create branches. By choosing splits that maximize Information Gain (or equivalently minimize entropy), a decision tree can effectively categorize data, leading to better classification or regression models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Import Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import tree\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Load and Explore the Iris Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = load_iris()\n",
    "df = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "df['target'] = iris.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Calculate Entropy\n",
    "To calculate the `entropy` we need to:\n",
    "- First, extract the target variable `y` from your dataset (like the 'target' column in the Iris dataset).\n",
    "- Then, call `calculate_entropy(y)` to get the entropy.\n",
    "\n",
    "This function calculates the entropy of a given target variable `y`. It works by first determining the unique classes in `y`, then computes the probability of each class, and uses this probability to calculate the entropy. This is a crucial step in understanding the disorder or uncertainty in the dataset, a fundamental concept in information theory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_entropy(y):\n",
    "    class_labels = np.unique(y)\n",
    "    entropy = 0\n",
    "    for label in class_labels:\n",
    "        probability = len(y[y == label]) / len(y)\n",
    "        entropy -= probability * np.log2(probability)\n",
    "    return entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is your observastion about the calculated Entropy?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The entropy function calculates the uncertainty in the target labels by summing the probabilities of all unique class labels. Higher entropy occurs when the labels are more evenly distributed, indicating greater uncertainty in classification. Conversely, lower entropy suggests that one class dominates, making the data more predictable. For the Iris dataset, a perfectly pure subset will have an entropy of 0, while the entire dataset, with three fairly balanced species, will have an entropy close to 1.5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Calculate Information Gain\n",
    "There are three steps for calculating the Information Gain:\n",
    "1. Compute Overall Entropy: Use the entropy function from Step 3 on the entire target dataset.\n",
    "2. Calculate Weighted Entropy for Each Attribute: For each unique value in the attribute, partition the dataset and calculate its entropy. Then calculate the weighted sum of these entropies, where the weights are the proportions of instances in each partition.\n",
    "3. Compute Information Gain: Subtract the weighted entropy of the split from the original entropy.\n",
    "\n",
    "The attribute with the highest Information Gain is generally chosen for splitting, as it provides the most significant reduction in uncertainty. This step is critical in constructing an effective decision tree, as it directly influences the structure and depth of the tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_information_gain(df, attribute, target_name):\n",
    "    total_entropy = calculate_entropy(df[target_name])\n",
    "    values, counts = np.unique(df[attribute], return_counts=True)\n",
    "    weighted_entropy = sum((counts[i] / sum(counts)) * calculate_entropy(df.where(df[attribute] == values[i]).dropna()[target_name]) for i in range(len(values)))\n",
    "    information_gain = total_entropy - weighted_entropy\n",
    "    return information_gain\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discuss your findings here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "The information gain function measures how well an attribute splits the data by comparing the total entropy before the split with the weighted entropy after the split. Attributes with higher information gain create more homogeneous subsets, reducing uncertainty about the target class. In decision trees, the attribute with the highest information gain is selected for splitting at each node, leading to more accurate classification. This ensures the tree focuses on the most informative features, improving prediction efficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Apply Entropy and Information Gain on a different dataset\n",
    "\n",
    "Your task is to choose a new dataset and implement what you learned in `Part 1` on this new dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, I have taken a dataset called titanic dataset from kaggle. i have downloaded that dataset from kaggle. The goal is to predict whether a passenger survived based on features like gender, class, and fare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Pclass  \\\n",
       "0            1         0       3   \n",
       "1            2         1       1   \n",
       "2            3         1       3   \n",
       "3            4         1       1   \n",
       "4            5         0       3   \n",
       "\n",
       "                                                Name     Sex   Age  SibSp  \\\n",
       "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
       "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
       "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
       "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
       "4                           Allen, Mr. William Henry    male  35.0      0   \n",
       "\n",
       "   Parch            Ticket     Fare Cabin Embarked  \n",
       "0      0         A/5 21171   7.2500   NaN        S  \n",
       "1      0          PC 17599  71.2833   C85        C  \n",
       "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
       "3      0            113803  53.1000  C123        S  \n",
       "4      0            373450   8.0500   NaN        S  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Loading the dataset \n",
    "import pandas as pd\n",
    "file_path = \"C:\\\\Users\\\\apoor\\\\Downloads\\\\titanic dataset\\\\Titanic-Dataset.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: Implement Entropy and Information Gain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropy of the target (Survived): 0.9607079018756469\n"
     ]
    }
   ],
   "source": [
    "#Implementing the entropy\n",
    "import numpy as np\n",
    "# Entropy function\n",
    "def calculate_entropy(y):\n",
    "    class_labels = np.unique(y)\n",
    "    entropy = 0\n",
    "    for label in class_labels:\n",
    "        probability = len(y[y == label]) / len(y)\n",
    "        entropy -= probability * np.log2(probability)\n",
    "    return entropy\n",
    "# Calculating entropy of the 'Survived' column\n",
    "target_entropy = calculate_entropy(df['Survived'])\n",
    "print(f\"Entropy of the target (Survived): {target_entropy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Information Gain for 'Sex': 0.2176601066606142\n",
      "Information Gain for 'Pclass': 0.0838310452960116\n"
     ]
    }
   ],
   "source": [
    "#Implementing the information gain\n",
    "# Information Gain function\n",
    "def calculate_information_gain(df, attribute, target_name='Survived'):\n",
    "    total_entropy = calculate_entropy(df[target_name])\n",
    "    values, counts = np.unique(df[attribute], return_counts=True)\n",
    "# Calculating weighted entropy\n",
    "    weighted_entropy = sum(\n",
    "        (counts[i] / sum(counts)) * calculate_entropy(\n",
    "            df[df[attribute] == values[i]][target_name]\n",
    "        )\n",
    "        for i in range(len(values))\n",
    "    )\n",
    "    information_gain = total_entropy - weighted_entropy\n",
    "    return information_gain\n",
    "# Calculating Information Gain for 'Sex'\n",
    "info_gain_sex = calculate_information_gain(df, 'Sex')\n",
    "print(f\"Information Gain for 'Sex': {info_gain_sex}\")\n",
    "# Calculating Information Gain for 'Pclass'\n",
    "info_gain_pclass = calculate_information_gain(df, 'Pclass')\n",
    "print(f\"Information Gain for 'Pclass': {info_gain_pclass}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: Discuss your findings in detail\n",
    "Provide detailed explanation and discussion about your findings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Entropy of the Target (Survived)\n",
    "The entropy of the 'Survived' column reflects the randomness or uncertainty in the dataset.\n",
    "In our case:The entropy is close to 1, indicating that the survival outcomes are fairly mixed (not entirely predictable).\n",
    "This suggests that the dataset contains both survivors and non-survivors in somewhat balanced proportions.\n",
    "2. Information Gain for 'Sex'\n",
    "The information gain for 'Sex' is relatively high (~0.217), meaning that gender is a strong predictor of survival.\n",
    "This aligns with the historical fact that women had higher survival rates during the Titanic disaster (more females survived than males).\n",
    "3. Information Gain for 'Pclass'\n",
    "The information gain for 'Pclass' is lower (~0.083), but it still provides some predictive power.\n",
    "This indicates that passenger class also impacts survival: passengers in higher classes had better survival chances, but not as strongly as gender.\n",
    "4. Conclusion\n",
    "Gender (sex) is the most important feature in predicting survival, with higher information gain compared to class.\n",
    "This suggests that any decision tree-based model trained on this dataset would likely first split the data based on Sex before using other attributes like Pclass.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission\n",
    "Submit a link to your completed Jupyter Notebook file hosted on your private GitHub repository through the submission link in Blackboard."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
